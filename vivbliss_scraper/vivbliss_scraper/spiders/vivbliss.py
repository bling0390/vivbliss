import scrapy
from vivbliss_scraper.items import VivblissItem, CategoryItem, ProductItem
import logging
import time
from urllib.parse import urljoin
import re
from datetime import datetime

# å¯¼å…¥è¾…åŠ©å·¥å…·
from vivbliss_scraper.utils.extraction_helpers import (
    CategoryExtractor, ProductExtractor, LinkDiscovery, DataValidator
)
from vivbliss_scraper.utils.spider_helpers import (
    SpiderStats, RequestBuilder, ResponseAnalyzer, LoggingHelper,
    timing_decorator, error_handler
)


class VivblissSpider(scrapy.Spider):
    name = 'vivbliss'
    allowed_domains = ['vivbliss.com']
    start_urls = ['https://vivbliss.com']
    
    def __init__(self, *args, **kwargs):
        super(VivblissSpider, self).__init__(*args, **kwargs)
        self.total_items = 0
        self.start_time = time.time()
        
        # åˆå§‹åŒ–ç»Ÿè®¡ç®¡ç†å™¨
        self.stats_manager = SpiderStats()
        
        # åˆå§‹åŒ–æå–å™¨
        self.category_extractor = CategoryExtractor()
        self.product_extractor = ProductExtractor()
        self.link_discovery = LinkDiscovery()
    
    custom_settings = {
        'DOWNLOAD_DELAY': 2,  # Increased from 1 to 2 seconds
        'CONCURRENT_REQUESTS': 2,
        'AUTOTHROTTLE_ENABLED': True,
        'AUTOTHROTTLE_TARGET_CONCURRENCY': 1.5,  # Reduced for gentler crawling
        'AUTOTHROTTLE_MAX_DELAY': 10,
        'RANDOMIZE_DOWNLOAD_DELAY': 0.5,
        'LOG_LEVEL': 'INFO',
    }
    
    def start_requests(self):
        """Override start_requests to add initial logging"""
        # ä½¿ç”¨æ—¥å¿—è¾…åŠ©å·¥å…·è®°å½•å¯åŠ¨ä¿¡æ¯
        config = {
            'allowed_domains': self.allowed_domains,
            'start_urls': self.start_urls,
            'ROBOTSTXT_OBEY': getattr(self.settings, 'ROBOTSTXT_OBEY', 'æœªè®¾ç½®'),
            'DOWNLOAD_DELAY': getattr(self.settings, 'DOWNLOAD_DELAY', 'æœªè®¾ç½®'),
            'CONCURRENT_REQUESTS': getattr(self.settings, 'CONCURRENT_REQUESTS', 'æœªè®¾ç½®'),
            'AUTOTHROTTLE_ENABLED': getattr(self.settings, 'AUTOTHROTTLE_ENABLED', 'æœªè®¾ç½®')
        }
        
        LoggingHelper.log_spider_start(self.logger, self.name, config)
        
        for url in self.start_urls:
            self.logger.info(f'ğŸ“¤ å‘é€è¯·æ±‚åˆ°: {url}')
            self.stats_manager.increment('requests_sent')
            yield scrapy.Request(url, self.parse)
            
    def closed(self, reason):
        """Called when the spider is closed"""
        # ä½¿ç”¨æ—¥å¿—è¾…åŠ©å·¥å…·è®°å½•ç»“æŸä¿¡æ¯
        LoggingHelper.log_spider_end(self.logger, self.name, self.stats_manager)
        self.logger.info(f'   ç»“æŸåŸå› : {reason}')

    def parse(self, response):
        # Log detailed response information
        self.logger.info(f'\n=== å¼€å§‹è§£æé¡µé¢ ===')
        self.logger.info(f'URL: {response.url}')
        self.logger.info(f'çŠ¶æ€ç : {response.status}')
        self.logger.info(f'å“åº”å¤§å°: {len(response.body)} bytes')
        self.logger.info(f'Content-Type: {response.headers.get("Content-Type", b"unknown").decode()}')
        
        # Log request delay information
        download_delay = getattr(self.settings, 'DOWNLOAD_DELAY', 1)
        self.logger.info(f'å½“å‰ä¸‹è½½å»¶è¿Ÿ: {download_delay} ç§’')
        
        # ğŸ¯ ä¼˜å…ˆå‘ç°å’Œçˆ¬å–åˆ†ç±»
        self.logger.info(f'ğŸ” å¼€å§‹æœç´¢äº§å“åˆ†ç±»...')
        for request in self.discover_categories(response):
            yield request
        
        # Start processing time
        start_time = time.time()
        
        # Multiple possible selectors for articles
        article_selectors = [
            'article',
            'div.post',
            'div.article',
            'div.blog-post',
            'div.entry'
        ]
        
        articles = None
        for selector in article_selectors:
            articles = response.css(selector)
            if articles:
                self.logger.info(f'Found {len(articles)} articles using selector: {selector}')
                break
        
        if not articles:
            self.logger.warning(f'âŒ åœ¨é¡µé¢ {response.url} ä¸Šæœªæ‰¾åˆ°ä»»ä½•æ–‡ç« ')
            self.logger.warning(f'é¡µé¢å†…å®¹é•¿åº¦: {len(response.text)} å­—ç¬¦')
            
            # Log page structure for debugging
            self.logger.debug(f'é¡µé¢æ ‡é¢˜: {response.css("title::text").get()}')
            self.logger.debug(f'é¡µé¢ä¸»è¦æ ‡ç­¾æ•°é‡: h1={len(response.css("h1"))}, h2={len(response.css("h2"))}, div={len(response.css("div"))}')
            return
        
        extracted_items = 0
        skipped_items = 0
        
        self.logger.info(f'ğŸ”„ å¼€å§‹å¤„ç† {len(articles)} ç¯‡æ–‡ç« ...')
        
        for i, article in enumerate(articles, 1):
            self.logger.debug(f'å¤„ç†ç¬¬ {i}/{len(articles)} ç¯‡æ–‡ç« ')
            item = VivblissItem()
            
            # Try multiple selectors for title
            title = (article.css('h2 a::text').get() or 
                    article.css('h2::text').get() or
                    article.css('h3 a::text').get() or
                    article.css('.title::text').get())
            
            # Try multiple selectors for URL
            url = (article.css('h2 a::attr(href)').get() or
                  article.css('h3 a::attr(href)').get() or
                  article.css('a::attr(href)').get())
            
            # Try multiple selectors for content
            content = (article.css('div.content::text').get() or
                      article.css('div.excerpt::text').get() or
                      article.css('p::text').get() or
                      article.css('::text').getall())
            
            if isinstance(content, list):
                content = ' '.join(content).strip()
            
            # Try multiple selectors for date
            date = (article.css('time::text').get() or
                   article.css('time::attr(datetime)').get() or
                   article.css('.date::text').get() or
                   article.css('.published::text').get())
            
            # Try multiple selectors for category
            category = (article.css('span.category::text').get() or
                       article.css('.category a::text').get() or
                       article.css('.tag::text').get() or
                       'Uncategorized')
            
            # Only yield item if we have at least title and URL
            if title and url:
                item['title'] = title.strip()
                item['url'] = response.urljoin(url)
                item['content'] = content.strip() if content else ''
                item['date'] = date.strip() if date else ''
                item['category'] = category.strip()
                
                # Log extracted item details
                self.logger.info(f'âœ… æå–æ–‡ç«  #{i}:')
                self.logger.info(f'   æ ‡é¢˜: {item["title"][:50]}...' if len(item['title']) > 50 else f'   æ ‡é¢˜: {item["title"]}')
                self.logger.info(f'   URL: {item["url"]}')
                self.logger.info(f'   åˆ†ç±»: {item["category"]}')
                self.logger.info(f'   æ—¥æœŸ: {item["date"]}')
                self.logger.info(f'   å†…å®¹é•¿åº¦: {len(item["content"])} å­—ç¬¦')
                
                extracted_items += 1
                self.total_items += 1
                yield item
            else:
                self.logger.warning(f'âŒ è·³è¿‡ç¬¬ {i} ç¯‡æ–‡ç«  - ç¼ºå°‘æ ‡é¢˜æˆ–URL')
                self.logger.warning(f'   æ ‡é¢˜: {title or "(ç©º)"}  URL: {url or "(ç©º)"}')
                skipped_items += 1

        # Multiple pagination selectors
        pagination_selectors = [
            'div.pagination a.next::attr(href)',
            'a.next::attr(href)',
            'a[rel="next"]::attr(href)',
            '.pagination a[aria-label="Next"]::attr(href)',
            'a:contains("Next")::attr(href)'
        ]
        
        # Log processing summary
        processing_time = time.time() - start_time
        self.logger.info(f'\n=== é¡µé¢å¤„ç†å®Œæˆ ===')
        self.logger.info(f'âœ… æˆåŠŸæå–: {extracted_items} ç¯‡æ–‡ç« ')
        self.logger.info(f'âŒ è·³è¿‡æ–‡ç« : {skipped_items} ç¯‡')
        self.logger.info(f'â±ï¸  å¤„ç†è€—æ—¶: {processing_time:.2f} ç§’')
        self.logger.info(f'ğŸ“Š æå–æ•ˆç‡: {extracted_items/processing_time:.1f} æ–‡ç« /ç§’' if processing_time > 0 else 'ğŸ“Š æå–æ•ˆç‡: N/A')
        
        # Enhanced pagination handling
        next_page = None
        for i, selector in enumerate(pagination_selectors, 1):
            next_page = response.css(selector).get()
            if next_page:
                self.logger.info(f'ğŸ”— å‘ç°ä¸‹ä¸€é¡µé“¾æ¥ (é€‰æ‹©å™¨ #{i}): {next_page}')
                full_next_url = response.urljoin(next_page)
                self.logger.info(f'ğŸ”— å®Œæ•´ä¸‹ä¸€é¡µURL: {full_next_url}')
                
                # Check if we should continue pagination
                if extracted_items > 0:  # Only continue if we found articles
                    self.logger.info(f'â¡ï¸  ç»§ç»­çˆ¬å–ä¸‹ä¸€é¡µ...')
                    yield response.follow(next_page, self.parse)
                else:
                    self.logger.warning(f'âš ï¸  æœ¬é¡µæœªæå–åˆ°æ–‡ç« ï¼Œåœæ­¢åˆ†é¡µçˆ¬å–')
                break
        
        if not next_page:
            self.logger.info(f'ğŸ æœªæ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥ï¼Œå¯èƒ½å·²åˆ°è¾¾æœ€åä¸€é¡µ')
            self.logger.debug(f'å°è¯•çš„åˆ†é¡µé€‰æ‹©å™¨: {", ".join(pagination_selectors)}')
    
    @timing_decorator
    @error_handler(default_return=[])
    def discover_categories(self, response):
        """å‘ç°å¹¶çˆ¬å–ç½‘ç«™åˆ†ç±»"""
        self.logger.info(f'ğŸ” æ­£åœ¨åˆ†æé¡µé¢ç»“æ„ï¼Œå¯»æ‰¾åˆ†ç±»å¯¼èˆª...')
        
        # ä½¿ç”¨é“¾æ¥å‘ç°å·¥å…·
        discovered_links = self.link_discovery.discover_category_links(response)
        
        # è®°å½•å‘ç°ç»“æœ
        LoggingHelper.log_discovery_results(self.logger, 'åˆ†ç±»', discovered_links)
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats_manager.increment('categories_discovered', len(discovered_links))
        
        if not discovered_links:
            self.logger.warning(f'âš ï¸  æœªå‘ç°ä»»ä½•åˆ†ç±»é“¾æ¥ï¼Œå°†å°è¯•é€šç”¨äº§å“å‘ç°')
            # å¦‚æœæ²¡æœ‰å‘ç°åˆ†ç±»ï¼Œå°è¯•ç›´æ¥å¯»æ‰¾äº§å“
            for request in self.discover_products(response):
                yield request
        else:
            # ä¸ºæ¯ä¸ªå‘ç°çš„åˆ†ç±»ç”Ÿæˆè¯·æ±‚
            for link_info in discovered_links:
                full_url = response.urljoin(link_info['url'])
                
                # ä½¿ç”¨è¯·æ±‚æ„å»ºå™¨åˆ›å»ºè¯·æ±‚
                request = RequestBuilder.build_category_request(
                    url=full_url,
                    category_info=link_info,
                    callback=self.parse_category
                )
                
                self.stats_manager.increment('requests_sent')
                yield request
    
    @timing_decorator
    @error_handler(default_return=[])
    def parse_category(self, response):
        """è§£æåˆ†ç±»é¡µé¢"""
        category_name = response.meta.get('category_name', 'æœªçŸ¥åˆ†ç±»')
        level = response.meta.get('level', 1)
        
        # è®°å½•é¡µé¢å¤„ç†å¼€å§‹
        LoggingHelper.log_page_processing(self.logger, response, f"åˆ†ç±»é¡µé¢: {category_name}")
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats_manager.increment('responses_received')
        
        # åˆ›å»ºåˆ†ç±»æ•°æ®é¡¹
        category_item = CategoryItem()
        
        # ä½¿ç”¨åˆ†ç±»æå–å™¨æå–æ•°æ®
        category_item['name'] = self.category_extractor.extract_category_name(response) or category_name
        category_item['url'] = response.url
        category_item['level'] = level
        category_item['created_at'] = datetime.now().isoformat()
        
        # æå–å…¶ä»–åˆ†ç±»ä¿¡æ¯
        category_item['description'] = self.category_extractor.extract_category_description(response)
        category_item['product_count'] = self.category_extractor.extract_product_count(response)
        category_item['image_url'] = self.category_extractor.extract_category_image(response)
        
        # æ„å»ºåˆ†ç±»è·¯å¾„
        parent_category = response.meta.get('parent_category')
        category_item['path'] = self.category_extractor.build_category_path(
            category_item['name'], parent_category
        )
        category_item['parent_category'] = parent_category
        
        # æå–URL slug
        from vivbliss_scraper.utils.spider_helpers import UrlPatternMatcher
        category_item['slug'] = UrlPatternMatcher.extract_category_slug(response.url)
        
        # SEOå…ƒæ•°æ®
        category_item['meta_title'] = response.css('title::text').get()
        category_item['meta_description'] = response.css('meta[name="description"]::attr(content)').get()
        
        # è®°å½•æå–çš„åˆ†ç±»ä¿¡æ¯
        LoggingHelper.log_item_extraction(self.logger, 'åˆ†ç±»', category_item)
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats_manager.add_category_stat({
            'name': category_item['name'],
            'url': category_item['url'],
            'level': category_item['level'],
            'product_count': category_item.get('product_count')
        })
        
        yield category_item
        
        # ğŸ” å¯»æ‰¾å­åˆ†ç±»
        self.logger.info(f'ğŸ” åœ¨åˆ†ç±» "{category_name}" ä¸­æœç´¢å­åˆ†ç±»...')
        subcategory_selectors = [
            '.subcategory a',
            '.sub-category a',
            '.category-children a',
            '.nested-category a',
            'ul.subcategories a'
        ]
        
        subcategories_found = 0
        for selector in subcategory_selectors:
            subcategory_links = response.css(selector)
            for link in subcategory_links:
                href = link.css('::attr(href)').get()
                text = link.css('::text').get()
                
                if href and text:
                    subcategories_found += 1
                    full_url = response.urljoin(href)
                    
                    self.logger.info(f'ğŸ”— å‘ç°å­åˆ†ç±»: "{text}" -> {full_url}')
                    
                    yield scrapy.Request(
                        url=full_url,
                        callback=self.parse_category,
                        meta={
                            'category_name': text.strip(),
                            'category_url': href,
                            'level': level + 1,
                            'parent_category': category_item['path']
                        }
                    )
        
        if subcategories_found > 0:
            self.logger.info(f'âœ… åœ¨åˆ†ç±» "{category_name}" ä¸­å‘ç° {subcategories_found} ä¸ªå­åˆ†ç±»')
        
        # ğŸ›ï¸  åœ¨å½“å‰åˆ†ç±»é¡µé¢ä¸­å¯»æ‰¾äº§å“
        self.logger.info(f'ğŸ›ï¸  åœ¨åˆ†ç±» "{category_name}" ä¸­æœç´¢äº§å“...')
        for request in self.discover_products(response, category_item['path']):
            yield request
        
        # ğŸ”„ å¤„ç†åˆ†ç±»é¡µé¢åˆ†é¡µ
        pagination_selectors = [
            '.pagination a.next::attr(href)',
            'a.next-page::attr(href)',
            'a[rel="next"]::attr(href)',
            '.pager a.next::attr(href)'
        ]
        
        for selector in pagination_selectors:
            next_page = response.css(selector).get()
            if next_page:
                self.logger.info(f'ğŸ“„ åˆ†ç±» "{category_name}" å‘ç°ä¸‹ä¸€é¡µ: {next_page}')
                yield scrapy.Request(
                    url=response.urljoin(next_page),
                    callback=self.parse_category,
                    meta=response.meta  # ä¼ é€’ç›¸åŒçš„å…ƒæ•°æ®
                )
                break
    
    @timing_decorator
    @error_handler(default_return=[])
    def discover_products(self, response, category_path=None):
        """åœ¨é¡µé¢ä¸­å‘ç°äº§å“é“¾æ¥"""
        self.logger.info(f'ğŸ›ï¸  å¼€å§‹æœç´¢äº§å“é“¾æ¥...')
        
        # ä½¿ç”¨é“¾æ¥å‘ç°å·¥å…·
        discovered_links = self.link_discovery.discover_product_links(response)
        
        # è®°å½•å‘ç°ç»“æœ
        LoggingHelper.log_discovery_results(self.logger, 'äº§å“', discovered_links)
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats_manager.increment('products_discovered', len(discovered_links))
        
        if discovered_links:
            # ä¸ºæ¯ä¸ªå‘ç°çš„äº§å“ç”Ÿæˆè¯·æ±‚
            for link_info in discovered_links:
                full_url = response.urljoin(link_info['url'])
                
                # ä½¿ç”¨è¯·æ±‚æ„å»ºå™¨åˆ›å»ºè¯·æ±‚
                request = RequestBuilder.build_product_request(
                    url=full_url,
                    product_info=link_info,
                    callback=self.parse_product,
                    category_path=category_path
                )
                
                self.stats_manager.increment('requests_sent')
                yield request
        else:
            self.logger.warning(f'âš ï¸  æœªå‘ç°ä»»ä½•äº§å“é“¾æ¥')
    
    @timing_decorator
    @error_handler(default_return=[])
    def parse_product(self, response):
        """è§£æäº§å“è¯¦æƒ…é¡µé¢"""
        category_path = response.meta.get('category_path', 'æœªåˆ†ç±»')
        
        # è®°å½•é¡µé¢å¤„ç†å¼€å§‹
        LoggingHelper.log_page_processing(self.logger, response, f"äº§å“é¡µé¢")
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats_manager.increment('responses_received')
        
        product_item = ProductItem()
        
        # åŸºç¡€ä¿¡æ¯
        product_item['url'] = response.url
        product_item['category_path'] = category_path
        product_item['created_at'] = datetime.now().isoformat()
        
        # ä½¿ç”¨äº§å“æå–å™¨æå–æ•°æ®
        product_item['name'] = self.product_extractor.extract_product_name(response)
        product_item['brand'] = self.product_extractor.extract_brand(response)
        product_item['sku'] = self.product_extractor.extract_sku(response)
        
        # æå–ä»·æ ¼ä¿¡æ¯
        price_info = self.product_extractor.extract_price_info(response)
        product_item['price'] = price_info['current_price']
        product_item['original_price'] = price_info['original_price']
        product_item['discount'] = price_info['discount']
        
        # æå–åº“å­˜ä¿¡æ¯
        stock_info = self.product_extractor.extract_stock_info(response)
        product_item['stock_status'] = stock_info['stock_status']
        product_item['stock_quantity'] = stock_info['stock_quantity']
        
        # æå–æè¿°å’Œå›¾ç‰‡
        product_item['description'] = self.product_extractor.extract_description(response)
        product_item['image_urls'] = self.product_extractor.extract_images(response)
        if product_item['image_urls']:
            product_item['thumbnail_url'] = product_item['image_urls'][0]
        
        # æå–è¯„åˆ†ä¿¡æ¯
        rating_info = self.product_extractor.extract_rating_info(response)
        product_item['rating'] = rating_info['rating']
        product_item['review_count'] = rating_info['review_count']
        
        # SEO å…ƒæ•°æ®
        product_item['meta_title'] = response.css('title::text').get()
        product_item['meta_description'] = response.css('meta[name="description"]::attr(content)').get()
        product_item['meta_keywords'] = response.css('meta[name="keywords"]::attr(content)').get()
        
        # è®°å½•æå–çš„äº§å“ä¿¡æ¯
        LoggingHelper.log_item_extraction(self.logger, 'äº§å“', product_item)
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats_manager.add_product_stat({
            'name': product_item['name'],
            'url': product_item['url'],
            'price': product_item.get('price'),
            'category_path': category_path
        })
        
        yield product_item