import scrapy
from vivbliss_scraper.items import VivblissItem, CategoryItem, ProductItem
import logging
import time
from urllib.parse import urljoin
import re
from datetime import datetime

# å¯¼å…¥è¾…åŠ©å·¥å…·
from vivbliss_scraper.utils.extraction_helpers import (
    CategoryExtractor, ProductExtractor, LinkDiscovery, DataValidator
)
from vivbliss_scraper.utils.spider_helpers import (
    SpiderStats, RequestBuilder, ResponseAnalyzer, LoggingHelper,
    timing_decorator, error_handler
)
from vivbliss_scraper.utils.media_extractor import MediaExtractor, MediaValidator


class VivblissSpider(scrapy.Spider):
    name = 'vivbliss'
    allowed_domains = ['vivbliss.com']
    start_urls = ['https://vivbliss.com']
    
    def __init__(self, *args, **kwargs):
        super(VivblissSpider, self).__init__(*args, **kwargs)
        self.total_items = 0
        self.start_time = time.time()
        
        # åˆå§‹åŒ–ç»Ÿè®¡ç®¡ç†å™¨
        self.stats_manager = SpiderStats()
        
        # åˆå§‹åŒ–æå–å™¨
        self.category_extractor = CategoryExtractor()
        self.product_extractor = ProductExtractor()
        self.link_discovery = LinkDiscovery()
        self.media_extractor = MediaExtractor()
        self.media_validator = MediaValidator()
    
    custom_settings = {
        'DOWNLOAD_DELAY': 2,  # Increased from 1 to 2 seconds
        'CONCURRENT_REQUESTS': 2,
        'AUTOTHROTTLE_ENABLED': True,
        'AUTOTHROTTLE_TARGET_CONCURRENCY': 1.5,  # Reduced for gentler crawling
        'AUTOTHROTTLE_MAX_DELAY': 10,
        'RANDOMIZE_DOWNLOAD_DELAY': 0.5,
        'LOG_LEVEL': 'INFO',
    }
    
    def start_requests(self):
        """Override start_requests to add initial logging"""
        # ä½¿ç”¨æ—¥å¿—è¾…åŠ©å·¥å…·è®°å½•å¯åŠ¨ä¿¡æ¯
        config = {
            'allowed_domains': self.allowed_domains,
            'start_urls': self.start_urls,
            'ROBOTSTXT_OBEY': getattr(self.settings, 'ROBOTSTXT_OBEY', 'æœªè®¾ç½®'),
            'DOWNLOAD_DELAY': getattr(self.settings, 'DOWNLOAD_DELAY', 'æœªè®¾ç½®'),
            'CONCURRENT_REQUESTS': getattr(self.settings, 'CONCURRENT_REQUESTS', 'æœªè®¾ç½®'),
            'AUTOTHROTTLE_ENABLED': getattr(self.settings, 'AUTOTHROTTLE_ENABLED', 'æœªè®¾ç½®')
        }
        
        LoggingHelper.log_spider_start(self.logger, self.name, config)
        
        for url in self.start_urls:
            self.logger.info(f'ğŸ“¤ å‘é€è¯·æ±‚åˆ°: {url}')
            self.stats_manager.increment('requests_sent')
            yield scrapy.Request(url, self.parse)
            
    def closed(self, reason):
        """Called when the spider is closed"""
        # ä½¿ç”¨æ—¥å¿—è¾…åŠ©å·¥å…·è®°å½•ç»“æŸä¿¡æ¯
        LoggingHelper.log_spider_end(self.logger, self.name, self.stats_manager)
        self.logger.info(f'   ç»“æŸåŸå› : {reason}')

    def parse(self, response):
        # Log detailed response information
        self.logger.info(f'\n=== å¼€å§‹è§£æé¡µé¢ ===')
        self.logger.info(f'URL: {response.url}')
        self.logger.info(f'çŠ¶æ€ç : {response.status}')
        self.logger.info(f'å“åº”å¤§å°: {len(response.body)} bytes')
        self.logger.info(f'Content-Type: {response.headers.get("Content-Type", b"unknown").decode()}')
        
        # Log request delay information
        download_delay = getattr(self.settings, 'DOWNLOAD_DELAY', 1)
        self.logger.info(f'å½“å‰ä¸‹è½½å»¶è¿Ÿ: {download_delay} ç§’')
        
        # ğŸ¯ ä¼˜å…ˆå‘ç°å’Œçˆ¬å–åˆ†ç±»
        self.logger.info(f'ğŸ” å¼€å§‹æœç´¢äº§å“åˆ†ç±»...')
        for request in self.discover_categories(response):
            yield request
        
        # Start processing time
        start_time = time.time()
        
        # Multiple possible selectors for articles
        article_selectors = [
            'article',
            'div.post',
            'div.article',
            'div.blog-post',
            'div.entry'
        ]
        
        articles = None
        for selector in article_selectors:
            articles = response.css(selector)
            if articles:
                self.logger.info(f'Found {len(articles)} articles using selector: {selector}')
                break
        
        if not articles:
            self.logger.warning(f'âŒ åœ¨é¡µé¢ {response.url} ä¸Šæœªæ‰¾åˆ°ä»»ä½•æ–‡ç« ')
            self.logger.warning(f'é¡µé¢å†…å®¹é•¿åº¦: {len(response.text)} å­—ç¬¦')
            
            # Log page structure for debugging
            self.logger.debug(f'é¡µé¢æ ‡é¢˜: {response.css("title::text").get()}')
            self.logger.debug(f'é¡µé¢ä¸»è¦æ ‡ç­¾æ•°é‡: h1={len(response.css("h1"))}, h2={len(response.css("h2"))}, div={len(response.css("div"))}')
            return
        
        extracted_items = 0
        skipped_items = 0
        
        self.logger.info(f'ğŸ”„ å¼€å§‹å¤„ç† {len(articles)} ç¯‡æ–‡ç« ...')
        
        for i, article in enumerate(articles, 1):
            self.logger.debug(f'å¤„ç†ç¬¬ {i}/{len(articles)} ç¯‡æ–‡ç« ')
            item = VivblissItem()
            
            # Try multiple selectors for title
            title = (article.css('h2 a::text').get() or 
                    article.css('h2::text').get() or
                    article.css('h3 a::text').get() or
                    article.css('.title::text').get())
            
            # Try multiple selectors for URL
            url = (article.css('h2 a::attr(href)').get() or
                  article.css('h3 a::attr(href)').get() or
                  article.css('a::attr(href)').get())
            
            # Try multiple selectors for content
            content = (article.css('div.content::text').get() or
                      article.css('div.excerpt::text').get() or
                      article.css('p::text').get() or
                      article.css('::text').getall())
            
            if isinstance(content, list):
                content = ' '.join(content).strip()
            
            # Try multiple selectors for date
            date = (article.css('time::text').get() or
                   article.css('time::attr(datetime)').get() or
                   article.css('.date::text').get() or
                   article.css('.published::text').get())
            
            # Try multiple selectors for category
            category = (article.css('span.category::text').get() or
                       article.css('.category a::text').get() or
                       article.css('.tag::text').get() or
                       'Uncategorized')
            
            # Only yield item if we have at least title and URL
            if title and url:
                item['title'] = title.strip()
                item['url'] = response.urljoin(url)
                item['content'] = content.strip() if content else ''
                item['date'] = date.strip() if date else ''
                item['category'] = category.strip()
                
                # ğŸ–¼ï¸ æå–åª’ä½“å†…å®¹
                media_content = self.extract_media_from_article(article, response)
                item['images'] = media_content.get('images', [])
                item['videos'] = media_content.get('videos', [])
                item['media_files'] = item['images'] + item['videos']
                item['media_count'] = len(item['media_files'])
                
                # Log extracted item details
                self.logger.info(f'âœ… æå–æ–‡ç«  #{i}:')
                self.logger.info(f'   æ ‡é¢˜: {item["title"][:50]}...' if len(item['title']) > 50 else f'   æ ‡é¢˜: {item["title"]}')
                self.logger.info(f'   URL: {item["url"]}')
                self.logger.info(f'   åˆ†ç±»: {item["category"]}')
                self.logger.info(f'   æ—¥æœŸ: {item["date"]}')
                self.logger.info(f'   å†…å®¹é•¿åº¦: {len(item["content"])} å­—ç¬¦')
                self.logger.info(f'   ğŸ“· å›¾ç‰‡æ•°é‡: {len(item["images"])}')
                self.logger.info(f'   ğŸ¥ è§†é¢‘æ•°é‡: {len(item["videos"])}')
                self.logger.info(f'   ğŸ“ åª’ä½“æ€»æ•°: {item["media_count"]}')
                
                extracted_items += 1
                self.total_items += 1
                yield item
            else:
                self.logger.warning(f'âŒ è·³è¿‡ç¬¬ {i} ç¯‡æ–‡ç«  - ç¼ºå°‘æ ‡é¢˜æˆ–URL')
                self.logger.warning(f'   æ ‡é¢˜: {title or "(ç©º)"}  URL: {url or "(ç©º)"}')
                skipped_items += 1

        # Multiple pagination selectors
        pagination_selectors = [
            'div.pagination a.next::attr(href)',
            'a.next::attr(href)',
            'a[rel="next"]::attr(href)',
            '.pagination a[aria-label="Next"]::attr(href)',
            'a:contains("Next")::attr(href)'
        ]
        
        # Log processing summary
        processing_time = time.time() - start_time
        self.logger.info(f'\n=== é¡µé¢å¤„ç†å®Œæˆ ===')
        self.logger.info(f'âœ… æˆåŠŸæå–: {extracted_items} ç¯‡æ–‡ç« ')
        self.logger.info(f'âŒ è·³è¿‡æ–‡ç« : {skipped_items} ç¯‡')
        self.logger.info(f'â±ï¸  å¤„ç†è€—æ—¶: {processing_time:.2f} ç§’')
        self.logger.info(f'ğŸ“Š æå–æ•ˆç‡: {extracted_items/processing_time:.1f} æ–‡ç« /ç§’' if processing_time > 0 else 'ğŸ“Š æå–æ•ˆç‡: N/A')
        
        # Enhanced pagination handling
        next_page = None
        for i, selector in enumerate(pagination_selectors, 1):
            next_page = response.css(selector).get()
            if next_page:
                self.logger.info(f'ğŸ”— å‘ç°ä¸‹ä¸€é¡µé“¾æ¥ (é€‰æ‹©å™¨ #{i}): {next_page}')
                full_next_url = response.urljoin(next_page)
                self.logger.info(f'ğŸ”— å®Œæ•´ä¸‹ä¸€é¡µURL: {full_next_url}')
                
                # Check if we should continue pagination
                if extracted_items > 0:  # Only continue if we found articles
                    self.logger.info(f'â¡ï¸  ç»§ç»­çˆ¬å–ä¸‹ä¸€é¡µ...')
                    yield response.follow(next_page, self.parse)
                else:
                    self.logger.warning(f'âš ï¸  æœ¬é¡µæœªæå–åˆ°æ–‡ç« ï¼Œåœæ­¢åˆ†é¡µçˆ¬å–')
                break
        
        if not next_page:
            self.logger.info(f'ğŸ æœªæ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥ï¼Œå¯èƒ½å·²åˆ°è¾¾æœ€åä¸€é¡µ')
            self.logger.debug(f'å°è¯•çš„åˆ†é¡µé€‰æ‹©å™¨: {", ".join(pagination_selectors)}')
    
    @timing_decorator
    @error_handler(default_return=[])
    def discover_categories(self, response):
        """å‘ç°å¹¶çˆ¬å–ç½‘ç«™åˆ†ç±»"""
        self.logger.info(f'ğŸ” æ­£åœ¨åˆ†æé¡µé¢ç»“æ„ï¼Œå¯»æ‰¾åˆ†ç±»å¯¼èˆª...')
        
        # ä½¿ç”¨é“¾æ¥å‘ç°å·¥å…·
        discovered_links = self.link_discovery.discover_category_links(response)
        
        # è®°å½•å‘ç°ç»“æœ
        LoggingHelper.log_discovery_results(self.logger, 'åˆ†ç±»', discovered_links)
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats_manager.increment('categories_discovered', len(discovered_links))
        
        if not discovered_links:
            self.logger.warning(f'âš ï¸  æœªå‘ç°ä»»ä½•åˆ†ç±»é“¾æ¥ï¼Œå°†å°è¯•é€šç”¨äº§å“å‘ç°')
            # å¦‚æœæ²¡æœ‰å‘ç°åˆ†ç±»ï¼Œå°è¯•ç›´æ¥å¯»æ‰¾äº§å“
            for request in self.discover_products(response):
                yield request
        else:
            # ä¸ºæ¯ä¸ªå‘ç°çš„åˆ†ç±»ç”Ÿæˆè¯·æ±‚
            for link_info in discovered_links:
                full_url = response.urljoin(link_info['url'])
                
                # ä½¿ç”¨è¯·æ±‚æ„å»ºå™¨åˆ›å»ºè¯·æ±‚
                request = RequestBuilder.build_category_request(
                    url=full_url,
                    category_info=link_info,
                    callback=self.parse_category
                )
                
                self.stats_manager.increment('requests_sent')
                yield request
    
    @timing_decorator
    @error_handler(default_return=[])
    def parse_category(self, response):
        """è§£æåˆ†ç±»é¡µé¢"""
        category_name = response.meta.get('category_name', 'æœªçŸ¥åˆ†ç±»')
        level = response.meta.get('level', 1)
        
        # è®°å½•é¡µé¢å¤„ç†å¼€å§‹
        LoggingHelper.log_page_processing(self.logger, response, f"åˆ†ç±»é¡µé¢: {category_name}")
        
        # è®°å½•è°ƒåº¦å™¨çŠ¶æ€
        self.log_scheduler_status()
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats_manager.increment('responses_received')
        
        # åˆ›å»ºåˆ†ç±»æ•°æ®é¡¹
        category_item = CategoryItem()
        
        # ä½¿ç”¨åˆ†ç±»æå–å™¨æå–æ•°æ®
        category_item['name'] = self.category_extractor.extract_category_name(response) or category_name
        category_item['url'] = response.url
        category_item['level'] = level
        category_item['created_at'] = datetime.now().isoformat()
        
        # æå–å…¶ä»–åˆ†ç±»ä¿¡æ¯
        category_item['description'] = self.category_extractor.extract_category_description(response)
        category_item['product_count'] = self.category_extractor.extract_product_count(response)
        category_item['image_url'] = self.category_extractor.extract_category_image(response)
        
        # æ„å»ºåˆ†ç±»è·¯å¾„
        parent_category = response.meta.get('parent_category')
        category_item['path'] = self.category_extractor.build_category_path(
            category_item['name'], parent_category
        )
        category_item['parent_category'] = parent_category
        
        # æå–URL slug
        from vivbliss_scraper.utils.spider_helpers import UrlPatternMatcher
        category_item['slug'] = UrlPatternMatcher.extract_category_slug(response.url)
        
        # SEOå…ƒæ•°æ®
        category_item['meta_title'] = response.css('title::text').get()
        category_item['meta_description'] = response.css('meta[name="description"]::attr(content)').get()
        
        # è®°å½•æå–çš„åˆ†ç±»ä¿¡æ¯
        LoggingHelper.log_item_extraction(self.logger, 'åˆ†ç±»', category_item)
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats_manager.add_category_stat({
            'name': category_item['name'],
            'url': category_item['url'],
            'level': category_item['level'],
            'product_count': category_item.get('product_count')
        })
        
        yield category_item
        
        # ğŸ” å¯»æ‰¾å­åˆ†ç±»
        self.logger.info(f'ğŸ” åœ¨åˆ†ç±» "{category_name}" ä¸­æœç´¢å­åˆ†ç±»...')
        subcategory_selectors = [
            '.subcategory a',
            '.sub-category a',
            '.category-children a',
            '.nested-category a',
            'ul.subcategories a'
        ]
        
        subcategories_found = 0
        for selector in subcategory_selectors:
            subcategory_links = response.css(selector)
            for link in subcategory_links:
                href = link.css('::attr(href)').get()
                text = link.css('::text').get()
                
                if href and text:
                    subcategories_found += 1
                    full_url = response.urljoin(href)
                    
                    self.logger.info(f'ğŸ”— å‘ç°å­åˆ†ç±»: "{text}" -> {full_url}')
                    
                    yield scrapy.Request(
                        url=full_url,
                        callback=self.parse_category,
                        meta={
                            'category_name': text.strip(),
                            'category_url': href,
                            'level': level + 1,
                            'parent_category': category_item['path']
                        }
                    )
        
        if subcategories_found > 0:
            self.logger.info(f'âœ… åœ¨åˆ†ç±» "{category_name}" ä¸­å‘ç° {subcategories_found} ä¸ªå­åˆ†ç±»')
        
        # ğŸ›ï¸  åœ¨å½“å‰åˆ†ç±»é¡µé¢ä¸­å¯»æ‰¾äº§å“
        self.logger.info(f'ğŸ›ï¸  åœ¨åˆ†ç±» "{category_name}" ä¸­æœç´¢äº§å“...')
        
        # è·å–è°ƒåº¦å™¨æ§åˆ¶çš„äº§å“è¯·æ±‚
        for request in self.discover_products_with_priority(response, category_path):
            yield request
        
        # ğŸ”„ å¤„ç†åˆ†ç±»é¡µé¢åˆ†é¡µ
        pagination_selectors = [
            '.pagination a.next::attr(href)',
            'a.next-page::attr(href)',
            'a[rel="next"]::attr(href)',
            '.pager a.next::attr(href)'
        ]
        
        for selector in pagination_selectors:
            next_page = response.css(selector).get()
            if next_page:
                self.logger.info(f'ğŸ“„ åˆ†ç±» "{category_name}" å‘ç°ä¸‹ä¸€é¡µ: {next_page}')
                yield scrapy.Request(
                    url=response.urljoin(next_page),
                    callback=self.parse_category,
                    meta=response.meta  # ä¼ é€’ç›¸åŒçš„å…ƒæ•°æ®
                )
                break
    
    @timing_decorator
    @error_handler(default_return=[])
    def discover_products(self, response, category_path=None):
        """åœ¨é¡µé¢ä¸­å‘ç°äº§å“é“¾æ¥"""
        self.logger.info(f'ğŸ›ï¸  å¼€å§‹æœç´¢äº§å“é“¾æ¥...')
        
        # ä½¿ç”¨é“¾æ¥å‘ç°å·¥å…·
        discovered_links = self.link_discovery.discover_product_links(response)
        
        # è®°å½•å‘ç°ç»“æœ
        LoggingHelper.log_discovery_results(self.logger, 'äº§å“', discovered_links)
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats_manager.increment('products_discovered', len(discovered_links))
        
        if discovered_links:
            # ä¸ºæ¯ä¸ªå‘ç°çš„äº§å“ç”Ÿæˆè¯·æ±‚
            for link_info in discovered_links:
                full_url = response.urljoin(link_info['url'])
                
                # ä½¿ç”¨è¯·æ±‚æ„å»ºå™¨åˆ›å»ºè¯·æ±‚
                request = RequestBuilder.build_product_request(
                    url=full_url,
                    product_info=link_info,
                    callback=self.parse_product,
                    category_path=category_path
                )
                
                self.stats_manager.increment('requests_sent')
                yield request
        else:
            self.logger.warning(f'âš ï¸  æœªå‘ç°ä»»ä½•äº§å“é“¾æ¥')
    
    @timing_decorator
    @error_handler(default_return=[])
    def parse_product(self, response):
        """è§£æäº§å“è¯¦æƒ…é¡µé¢"""
        category_path = response.meta.get('category_path', 'æœªåˆ†ç±»')
        
        # è®°å½•é¡µé¢å¤„ç†å¼€å§‹
        LoggingHelper.log_page_processing(self.logger, response, f"äº§å“é¡µé¢")
        
        # è®°å½•è°ƒåº¦å™¨çŠ¶æ€
        self.log_scheduler_status()
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats_manager.increment('responses_received')
        
        product_item = ProductItem()
        
        # åŸºç¡€ä¿¡æ¯
        product_item['url'] = response.url
        product_item['category_path'] = category_path
        product_item['created_at'] = datetime.now().isoformat()
        
        # ä½¿ç”¨äº§å“æå–å™¨æå–æ•°æ®
        product_item['name'] = self.product_extractor.extract_product_name(response)
        product_item['brand'] = self.product_extractor.extract_brand(response)
        product_item['sku'] = self.product_extractor.extract_sku(response)
        
        # æå–ä»·æ ¼ä¿¡æ¯
        price_info = self.product_extractor.extract_price_info(response)
        product_item['price'] = price_info['current_price']
        product_item['original_price'] = price_info['original_price']
        product_item['discount'] = price_info['discount']
        
        # æå–åº“å­˜ä¿¡æ¯
        stock_info = self.product_extractor.extract_stock_info(response)
        product_item['stock_status'] = stock_info['stock_status']
        product_item['stock_quantity'] = stock_info['stock_quantity']
        
        # æå–æè¿°å’Œå›¾ç‰‡
        product_item['description'] = self.product_extractor.extract_description(response)
        product_item['image_urls'] = self.product_extractor.extract_images(response)
        if product_item['image_urls']:
            product_item['thumbnail_url'] = product_item['image_urls'][0]
        
        # æå–è¯„åˆ†ä¿¡æ¯
        rating_info = self.product_extractor.extract_rating_info(response)
        product_item['rating'] = rating_info['rating']
        product_item['review_count'] = rating_info['review_count']
        
        # SEO å…ƒæ•°æ®
        product_item['meta_title'] = response.css('title::text').get()
        product_item['meta_description'] = response.css('meta[name="description"]::attr(content)').get()
        product_item['meta_keywords'] = response.css('meta[name="keywords"]::attr(content)').get()
        
        # è®°å½•æå–çš„äº§å“ä¿¡æ¯
        LoggingHelper.log_item_extraction(self.logger, 'äº§å“', product_item)
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats_manager.add_product_stat({
            'name': product_item['name'],
            'url': product_item['url'],
            'price': product_item.get('price'),
            'category_path': category_path
        })
        
        yield product_item
    
    def extract_media_from_article(self, article_selector, response):
        """ä»æ–‡ç« é€‰æ‹©å™¨ä¸­æå–åª’ä½“å†…å®¹"""
        try:
            # åˆ›å»ºä¸€ä¸ªä¸´æ—¶å“åº”å¯¹è±¡ç”¨äºåª’ä½“æå–
            article_html = article_selector.get()
            if not article_html:
                return {'images': [], 'videos': []}
            
            # ä½¿ç”¨åª’ä½“æå–å™¨æå–å†…å®¹
            from scrapy.http import HtmlResponse
            article_response = HtmlResponse(
                url=response.url,
                body=article_html.encode('utf-8'),
                encoding='utf-8'
            )
            
            # æå–å›¾ç‰‡å’Œè§†é¢‘
            images = self.extract_images_from_article(article_response)
            videos = self.extract_videos_from_article(article_response)
            
            # éªŒè¯åª’ä½“URLs
            validated_images = self.validate_media_urls(images, response)
            validated_videos = self.validate_media_urls(videos, response)
            
            self.logger.debug(f"ä»æ–‡ç« ä¸­æå–: {len(validated_images)} å¼ å›¾ç‰‡, {len(validated_videos)} ä¸ªè§†é¢‘")
            
            return {
                'images': validated_images,
                'videos': validated_videos
            }
            
        except Exception as e:
            self.logger.error(f"æå–åª’ä½“å†…å®¹æ—¶å‡ºé”™: {e}")
            return {'images': [], 'videos': []}
    
    def extract_images_from_article(self, response):
        """ä»æ–‡ç« ä¸­æå–å›¾ç‰‡URLs"""
        return self.media_extractor.extract_images_from_response(response)
    
    def extract_videos_from_article(self, response):
        """ä»æ–‡ç« ä¸­æå–è§†é¢‘URLs"""
        return self.media_extractor.extract_videos_from_response(response)
    
    def validate_media_urls(self, urls, response):
        """éªŒè¯åª’ä½“URLsçš„æœ‰æ•ˆæ€§"""
        if not urls:
            return []
        
        validated_urls = []
        for url in urls:
            try:
                # è½¬æ¢ä¸ºç»å¯¹URL
                absolute_url = response.urljoin(url.strip())
                
                # éªŒè¯URLæ ¼å¼å’Œç±»å‹
                if (self.media_validator.is_valid_image_url(absolute_url) or 
                    self.media_validator.is_valid_video_url(absolute_url)):
                    validated_urls.append(absolute_url)
                    
            except Exception as e:
                self.logger.debug(f"éªŒè¯åª’ä½“URLæ—¶å‡ºé”™: {url}, é”™è¯¯: {e}")
                continue
        
        return validated_urls
    
    def get_next_priority_request(self):
        """è·å–ä¸‹ä¸€ä¸ªä¼˜å…ˆçº§è¯·æ±‚"""
        try:
            return self.priority_scheduler.get_next_request()
        except Exception as e:
            self.logger.error(f"è·å–ä¼˜å…ˆçº§è¯·æ±‚æ—¶å‡ºé”™: {e}")
            return None
    
    def log_scheduler_status(self):
        """è®°å½•è°ƒåº¦å™¨çŠ¶æ€"""
        try:
            stats = self.priority_scheduler.get_scheduler_stats()
            self.logger.info(f"ğŸ¯ å½“å‰ä¼˜å…ˆç›®å½•: {stats.get('current_priority_directory', 'æ— ')}")
            
            # æ˜¾ç¤ºæ´»è·ƒç›®å½•
            active_dirs = stats.get('active_directories', [])
            if active_dirs:
                self.logger.info(f"ğŸ“ æ´»è·ƒç›®å½• ({len(active_dirs)}): {', '.join(active_dirs[:3])}{'...' if len(active_dirs) > 3 else ''}")
                
        except Exception as e:
            self.logger.error(f"è®°å½•è°ƒåº¦å™¨çŠ¶æ€æ—¶å‡ºé”™: {e}")
    
    @timing_decorator
    @error_handler(default_return=[])
    def parse_product_with_error_handling(self, response):
        """å¸¦æœ‰é”™è¯¯å¤„ç†çš„äº§å“è§£ææ–¹æ³•"""
        try:
            # è°ƒç”¨åŸæœ‰çš„äº§å“è§£ææ–¹æ³•
            for item in self.parse_product(response):
                yield item
                
        except Exception as e:
            # å¤„ç†é”™è¯¯æƒ…å†µ
            self.logger.error(f'âŒ äº§å“é¡µé¢è§£æå¤±è´¥: {response.url}, é”™è¯¯: {e}')
            
            # é€šçŸ¥è°ƒåº¦å™¨äº§å“å¤„ç†å¤±è´¥
            self.priority_scheduler.mark_product_failed(response.url)
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats_manager.increment('products_failed')