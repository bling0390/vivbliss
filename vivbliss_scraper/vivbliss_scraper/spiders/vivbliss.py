import scrapy
from vivbliss_scraper.items import VivblissItem
import logging
import time
from urllib.parse import urljoin


class VivblissSpider(scrapy.Spider):
    name = 'vivbliss'
    allowed_domains = ['vivbliss.com']
    start_urls = ['https://vivbliss.com']
    
    def __init__(self, *args, **kwargs):
        super(VivblissSpider, self).__init__(*args, **kwargs)
        self.total_items = 0
        self.start_time = time.time()
    
    custom_settings = {
        'DOWNLOAD_DELAY': 2,  # Increased from 1 to 2 seconds
        'CONCURRENT_REQUESTS': 2,
        'AUTOTHROTTLE_ENABLED': True,
        'AUTOTHROTTLE_TARGET_CONCURRENCY': 1.5,  # Reduced for gentler crawling
        'AUTOTHROTTLE_MAX_DELAY': 10,
        'RANDOMIZE_DOWNLOAD_DELAY': 0.5,
        'LOG_LEVEL': 'INFO',
    }
    
    def start_requests(self):
        """Override start_requests to add initial logging"""
        self.logger.info(f'\nğŸš€ å¼€å§‹çˆ¬å– {self.name} çˆ¬è™«')
        self.logger.info(f'ğŸ¯ ç›®æ ‡åŸŸå: {", ".join(self.allowed_domains)}')
        self.logger.info(f'ğŸ“‹ èµ·å§‹URLæ•°é‡: {len(self.start_urls)}')
        
        # Log current settings
        self.logger.info(f'âš™ï¸  çˆ¬è™«é…ç½®:')
        self.logger.info(f'   ROBOTSTXT_OBEY: {getattr(self.settings, "ROBOTSTXT_OBEY", "æœªè®¾ç½®")}')
        self.logger.info(f'   DOWNLOAD_DELAY: {getattr(self.settings, "DOWNLOAD_DELAY", "æœªè®¾ç½®")} ç§’')
        self.logger.info(f'   CONCURRENT_REQUESTS: {getattr(self.settings, "CONCURRENT_REQUESTS", "æœªè®¾ç½®")}')
        self.logger.info(f'   AUTOTHROTTLE_ENABLED: {getattr(self.settings, "AUTOTHROTTLE_ENABLED", "æœªè®¾ç½®")}')
        
        for url in self.start_urls:
            self.logger.info(f'ğŸ“¤ å‘é€è¯·æ±‚åˆ°: {url}')
            yield scrapy.Request(url, self.parse)
            
    def closed(self, reason):
        """Called when the spider is closed"""
        end_time = time.time()
        duration = end_time - self.start_time
        
        self.logger.info(f'\nğŸ çˆ¬è™« {self.name} ç»“æŸè¿è¡Œ')
        self.logger.info(f'ğŸ“Š çˆ¬å–ç»Ÿè®¡:')
        self.logger.info(f'   æ€»æå–æ–‡ç« : {self.total_items} ç¯‡')
        self.logger.info(f'   æ€»è€—æ—¶: {duration:.2f} ç§’')
        self.logger.info(f'   å¹³å‡é€Ÿåº¦: {self.total_items/duration:.2f} æ–‡ç« /ç§’' if duration > 0 else '   å¹³å‡é€Ÿåº¦: N/A')
        self.logger.info(f'   ç»“æŸåŸå› : {reason}')

    def parse(self, response):
        # Log detailed response information
        self.logger.info(f'\n=== å¼€å§‹è§£æé¡µé¢ ===')
        self.logger.info(f'URL: {response.url}')
        self.logger.info(f'çŠ¶æ€ç : {response.status}')
        self.logger.info(f'å“åº”å¤§å°: {len(response.body)} bytes')
        self.logger.info(f'Content-Type: {response.headers.get("Content-Type", b"unknown").decode()}')
        
        # Log request delay information
        download_delay = getattr(self.settings, 'DOWNLOAD_DELAY', 1)
        self.logger.info(f'å½“å‰ä¸‹è½½å»¶è¿Ÿ: {download_delay} ç§’')
        
        # Start processing time
        start_time = time.time()
        
        # Multiple possible selectors for articles
        article_selectors = [
            'article',
            'div.post',
            'div.article',
            'div.blog-post',
            'div.entry'
        ]
        
        articles = None
        for selector in article_selectors:
            articles = response.css(selector)
            if articles:
                self.logger.info(f'Found {len(articles)} articles using selector: {selector}')
                break
        
        if not articles:
            self.logger.warning(f'âŒ åœ¨é¡µé¢ {response.url} ä¸Šæœªæ‰¾åˆ°ä»»ä½•æ–‡ç« ')
            self.logger.warning(f'é¡µé¢å†…å®¹é•¿åº¦: {len(response.text)} å­—ç¬¦')
            
            # Log page structure for debugging
            self.logger.debug(f'é¡µé¢æ ‡é¢˜: {response.css("title::text").get()}')
            self.logger.debug(f'é¡µé¢ä¸»è¦æ ‡ç­¾æ•°é‡: h1={len(response.css("h1"))}, h2={len(response.css("h2"))}, div={len(response.css("div"))}')
            return
        
        extracted_items = 0
        skipped_items = 0
        
        self.logger.info(f'ğŸ”„ å¼€å§‹å¤„ç† {len(articles)} ç¯‡æ–‡ç« ...')
        
        for i, article in enumerate(articles, 1):
            self.logger.debug(f'å¤„ç†ç¬¬ {i}/{len(articles)} ç¯‡æ–‡ç« ')
            item = VivblissItem()
            
            # Try multiple selectors for title
            title = (article.css('h2 a::text').get() or 
                    article.css('h2::text').get() or
                    article.css('h3 a::text').get() or
                    article.css('.title::text').get())
            
            # Try multiple selectors for URL
            url = (article.css('h2 a::attr(href)').get() or
                  article.css('h3 a::attr(href)').get() or
                  article.css('a::attr(href)').get())
            
            # Try multiple selectors for content
            content = (article.css('div.content::text').get() or
                      article.css('div.excerpt::text').get() or
                      article.css('p::text').get() or
                      article.css('::text').getall())
            
            if isinstance(content, list):
                content = ' '.join(content).strip()
            
            # Try multiple selectors for date
            date = (article.css('time::text').get() or
                   article.css('time::attr(datetime)').get() or
                   article.css('.date::text').get() or
                   article.css('.published::text').get())
            
            # Try multiple selectors for category
            category = (article.css('span.category::text').get() or
                       article.css('.category a::text').get() or
                       article.css('.tag::text').get() or
                       'Uncategorized')
            
            # Only yield item if we have at least title and URL
            if title and url:
                item['title'] = title.strip()
                item['url'] = response.urljoin(url)
                item['content'] = content.strip() if content else ''
                item['date'] = date.strip() if date else ''
                item['category'] = category.strip()
                
                # Log extracted item details
                self.logger.info(f'âœ… æå–æ–‡ç«  #{i}:')
                self.logger.info(f'   æ ‡é¢˜: {item["title"][:50]}...' if len(item['title']) > 50 else f'   æ ‡é¢˜: {item["title"]}')
                self.logger.info(f'   URL: {item["url"]}')
                self.logger.info(f'   åˆ†ç±»: {item["category"]}')
                self.logger.info(f'   æ—¥æœŸ: {item["date"]}')
                self.logger.info(f'   å†…å®¹é•¿åº¦: {len(item["content"])} å­—ç¬¦')
                
                extracted_items += 1
                self.total_items += 1
                yield item
            else:
                self.logger.warning(f'âŒ è·³è¿‡ç¬¬ {i} ç¯‡æ–‡ç«  - ç¼ºå°‘æ ‡é¢˜æˆ–URL')
                self.logger.warning(f'   æ ‡é¢˜: {title or "(ç©º)"}  URL: {url or "(ç©º)"}')
                skipped_items += 1

        # Multiple pagination selectors
        pagination_selectors = [
            'div.pagination a.next::attr(href)',
            'a.next::attr(href)',
            'a[rel="next"]::attr(href)',
            '.pagination a[aria-label="Next"]::attr(href)',
            'a:contains("Next")::attr(href)'
        ]
        
        # Log processing summary
        processing_time = time.time() - start_time
        self.logger.info(f'\n=== é¡µé¢å¤„ç†å®Œæˆ ===')
        self.logger.info(f'âœ… æˆåŠŸæå–: {extracted_items} ç¯‡æ–‡ç« ')
        self.logger.info(f'âŒ è·³è¿‡æ–‡ç« : {skipped_items} ç¯‡')
        self.logger.info(f'â±ï¸  å¤„ç†è€—æ—¶: {processing_time:.2f} ç§’')
        self.logger.info(f'ğŸ“Š æå–æ•ˆç‡: {extracted_items/processing_time:.1f} æ–‡ç« /ç§’' if processing_time > 0 else 'ğŸ“Š æå–æ•ˆç‡: N/A')
        
        # Enhanced pagination handling
        next_page = None
        for i, selector in enumerate(pagination_selectors, 1):
            next_page = response.css(selector).get()
            if next_page:
                self.logger.info(f'ğŸ”— å‘ç°ä¸‹ä¸€é¡µé“¾æ¥ (é€‰æ‹©å™¨ #{i}): {next_page}')
                full_next_url = response.urljoin(next_page)
                self.logger.info(f'ğŸ”— å®Œæ•´ä¸‹ä¸€é¡µURL: {full_next_url}')
                
                # Check if we should continue pagination
                if extracted_items > 0:  # Only continue if we found articles
                    self.logger.info(f'â¡ï¸  ç»§ç»­çˆ¬å–ä¸‹ä¸€é¡µ...')
                    yield response.follow(next_page, self.parse)
                else:
                    self.logger.warning(f'âš ï¸  æœ¬é¡µæœªæå–åˆ°æ–‡ç« ï¼Œåœæ­¢åˆ†é¡µçˆ¬å–')
                break
        
        if not next_page:
            self.logger.info(f'ğŸ æœªæ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥ï¼Œå¯èƒ½å·²åˆ°è¾¾æœ€åä¸€é¡µ')
            self.logger.debug(f'å°è¯•çš„åˆ†é¡µé€‰æ‹©å™¨: {", ".join(pagination_selectors)}')